{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a58b908-009f-4300-8c56-8fffd5ab97c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import plotly.express as px\n",
    "from operator import itemgetter\n",
    "import statsmodels.api as sm\n",
    "\n",
    "\n",
    "def is_majority_below_threshold(data, threshold):\n",
    "    return (data < threshold).mean() > 0.5\n",
    "def normalisation(series):\n",
    "    return (series - series.min()) / (series.max() - series.min())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8a3d08b-a71d-49a2-ab3b-91a0d90aff77",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=pd.read_csv(\"e7-htr-currernt.csv\")\n",
    "fig = px.line(data,x=\"Timestamp\",y=\"HT R Phase Current\")\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fecda7e-767e-4c01-a850-44c70a4535ac",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig.update_xaxes(range=[1, 2880])\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b6d58a-7b0e-428a-932c-3486e39d63f7",
   "metadata": {},
   "source": [
    "We will be storing the individual data from each day into the dictionary daily_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4116e714-f6d0-4f7b-815f-a2b175b420eb",
   "metadata": {},
   "source": [
    "We will be creating the ideal day by taking the average of the best days as we can see from the graph in section 3 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76919349-7918-4027-82a0-8228b0160b79",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "total_days=280\n",
    "samples_per_day=288\n",
    "daily_data={}\n",
    "daily_data['Day_1']=data.iloc[0:222,1]\n",
    "for d in range(total_days):\n",
    "    start_index=d*samples_per_day+222\n",
    "    end_index=start_index+samples_per_day\n",
    "    day_data=data.iloc[start_index:end_index,1]\n",
    "    daily_data[f'Day_{d+2}'] = day_data\n",
    "    daily_data[f'Day_{d+2}']=daily_data[f'Day_{d+2}'].reset_index(drop=True)\n",
    "pd.set_option('display.max_rows', 1000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea7d2d2e-e7f2-463e-a33d-ad15625c2bec",
   "metadata": {},
   "source": [
    "We will be finding the root mean square error of all of the data present in each day with the values of the ideal day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ad2a4b5-d0b1-438a-9be0-37152ee1d394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "best_days_data=[daily_data['Day_3'],daily_data['Day_7'],daily_data['Day_8'],daily_data['Day_9']]\n",
    "ideal_day=(daily_data['Day_3']+daily_data['Day_9']+daily_data['Day_7']+daily_data['Day_8'])/4\n",
    "rms_errors = {}\n",
    "for day, data in daily_data.items():\n",
    "    if day=='Day_1':\n",
    "        continue\n",
    "    rms_errors[day] = np.sqrt(np.sum((daily_data[day].iloc[79:217] - ideal_day[79:217]) ** 2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "352564d4-815e-4556-93e5-d6b77c43baa1",
   "metadata": {},
   "source": [
    "We classify the data into good ,bad and missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2845efa5-2efa-49d1-b9f1-db506ee30efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "classification={\n",
    "    'good':[],\n",
    "    'bad':[],\n",
    "    'missing':[]\n",
    "}\n",
    "best_days_errors=[rms_errors['Day_3'],rms_errors['Day_7'],rms_errors['Day_8'],rms_errors['Day_9']]\n",
    "mean=np.mean([rms_errors['Day_3'],rms_errors['Day_7'],rms_errors['Day_8'],rms_errors['Day_9']])\n",
    "\n",
    "for day,data in daily_data.items():\n",
    "    if day=='Day_1':\n",
    "        continue\n",
    "    if  is_majority_below_threshold(daily_data[day].iloc[119:155],1.1 ) or rms_errors[day]>(7.5)*np.amax(best_days_errors):\n",
    "        classification['missing'].append(day)\n",
    "    elif rms_errors[day]>0.8*np.amin(best_days_errors) and rms_errors[day]<15.8*np.amin(best_days_errors):\n",
    "        classification['good'].append(day)\n",
    "    else:\n",
    "        classification['bad'].append(day)\n",
    "    \n",
    "num_good_days = len(classification['good'])\n",
    "num_bad_days = len(classification['bad'])\n",
    "num_missing_days = len(classification['missing'])\n",
    "\n",
    "print(f\"Number of Good Days: {num_good_days}\")\n",
    "print(f\"Number of Bad Days: {num_bad_days}\")\n",
    "print(f\"Number of Missing Days: {num_missing_days}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df48e448-61ce-4096-b0c5-ba1d3153a125",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c294c27-9f8b-4215-90a3-e243004c1648",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a05314a-0dd5-439a-a48d-38761e6a5d07",
   "metadata": {},
   "source": [
    "Now we group the good days data into a single dataframe and improve the quality by removing outliers and replacing them with ideal day values for the specific datapoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7618241c-803e-42eb-b8ac-7d6dceef2db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good_initial=pd.concat([daily_data[day] for day in classification['good']], axis=0).reset_index(drop=True)\n",
    "SSE=0\n",
    "n=0\n",
    "for day in classification['good']:\n",
    "    for i in range(288):\n",
    "        SSE=SSE+(daily_data[day].iloc[i] - ideal_day[i])**2\n",
    "        n=n+1\n",
    "s=(SSE/n-2)**0.5\n",
    "for day in classification['good']:\n",
    "    for i in range(288):\n",
    "        if (daily_data[day].iloc[i] - ideal_day[i])**2>2*s:\n",
    "            daily_data[day].iloc[i] = ideal_day[i]\n",
    "        else:\n",
    "            continue\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8555bf8-5597-462f-adaf-dc56d1c87584",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df=pd.concat([daily_data[day] for day in daily_data.keys()],axis=0).reset_index(drop=True)\n",
    "datacpy=pd.read_csv(\"e7-htr-currernt.csv\")\n",
    "fig = px.line(x=df.index,y=datacpy[\"HT R Phase Current\"].iloc[0:80862],title=\"Good Data After Improvement(zoom in)\")\n",
    "fig.add_scatter(x=df.index,y=df,fillcolor=\"black\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad90463-a081-41b4-95a2-b038a623c87e",
   "metadata": {},
   "source": [
    "Now we go ahead with the mlr model for the data ,adding hours, minutes and their higher and mutual powers as our features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35d0909-cb57-4a8c-a368-4467c5a4aade",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_good = pd.concat([daily_data[day] for day in classification['good']], axis=0).reset_index(drop=True)\n",
    "\n",
    "feature_names = {f'hours^{i}': [] for i in range(1, 5)}\n",
    "feature_names.update({f'minutes^{i}': [] for i in range(1, 5)})\n",
    "\n",
    "for h1 in range(1, 5):\n",
    "    for m1 in range(1, 5):\n",
    "        feature_names[f'hours^{h1}*minutes^{m1}'] = []\n",
    "\n",
    "print(feature_names)\n",
    "\n",
    "h=0\n",
    "m=0\n",
    "for p in range(80862):\n",
    "    total_minutes = p * 5  \n",
    "    h=(total_minutes // 60) % 24 \n",
    "    m=total_minutes % 60 \n",
    "    for q in range(1, 5):\n",
    "        feature_names[f'hours^{q}'].append(h ** q)\n",
    "        feature_names[f'minutes^{q}'].append(m ** q)\n",
    "       \n",
    "    for h1 in range(1, 5):\n",
    "        for m1 in range(1, 5):\n",
    "            feature_names[f'hours^{h1}*minutes^{m1}'].append((h ** h1) * (m ** m1))\n",
    "features_df = pd.DataFrame(feature_names)\n",
    "features_df = normalisation(features_df)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee55dd4a-1e00-4429-bee8-d9fd8ef30eb4",
   "metadata": {},
   "source": [
    "now we train our model with 80% of the good day data and eliminate the features with a p value over 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d4d8f51-0885-4c3a-a3fc-ebfaa1f7b15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "total_rows = len(df_good)\n",
    "train_end_index = int(total_rows * 0.8)\n",
    "\n",
    "train_data_y = df_good[:train_end_index]\n",
    "train_data_x=features_df[:train_end_index]\n",
    "train_data_y=train_data_y.astype(float)\n",
    "train_data_x=train_data_x.astype(float)\n",
    "\n",
    "mlr_question = sm.OLS(train_data_y,train_data_x).fit()\n",
    "p_values=mlr_question.pvalues\n",
    "eliminated_features = p_values[p_values > 0.05].index\n",
    "train_data_x_new = train_data_x.drop(columns=eliminated_features)\n",
    "\n",
    "mlr_2 = sm.OLS(train_data_y,train_data_x_new).fit()\n",
    "coefficients=mlr_2.params\n",
    "p_values_2=mlr_2.pvalues\n",
    "R_squared_2=mlr_2.rsquared\n",
    "\n",
    "eliminated_features_2 = p_values_2[p_values_2 > 0.05].index\n",
    "train_data_x_final = train_data_x_new.drop(columns=eliminated_features_2)\n",
    "mlr_final = sm.OLS(train_data_y,train_data_x_final).fit()\n",
    "\n",
    "coefficients=mlr_final.params\n",
    "p_values_final=mlr_final.pvalues\n",
    "R_squared_final=mlr_final.rsquared\n",
    "F_value_final=mlr_final.fvalue\n",
    "coefficients=mlr_final.params\n",
    "print(mlr_final.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ddccc7-b56d-49c0-9bdd-95c172117dd4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "da368c3a-db07-431c-ad3f-ddeecc47e22a",
   "metadata": {},
   "source": [
    "We predict the next 20% good day data and find the mean square error with the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4edf1171-4521-41d1-84d3-9de41e69e6ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_data_y = df_good[train_end_index:]\n",
    "validation_data_x1 = features_df[train_end_index:train_end_index + len(validation_data_y)]\n",
    "validation_data_x2=validation_data_x1.drop(columns=eliminated_features_2)\n",
    "validation_data_x=validation_data_x2.drop(columns=eliminated_features)\n",
    "y_predict=mlr_final.predict(validation_data_x)\n",
    "y_predict = y_predict.apply(lambda x: max(x, 0.1))\n",
    "mse_test_data=np.mean(np.square(y_predict-validation_data_y))\n",
    "print(mse_test_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d2eb90a-0e82-4b56-a677-d2f2bf406d3e",
   "metadata": {},
   "source": [
    "Now we use our model to replace the bad and missing days with values predicted by our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1b933-a1ee-480b-910a-5c4d40f3d51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bad_missing_data = pd.concat([daily_data[day] for day in classification['bad']+classification['missing']], axis=0).reset_index(drop=True)\n",
    "bad_missing_features = features_df.loc[:].drop(columns=eliminated_features)\n",
    "y_predict_bad_missing = mlr_final.predict(bad_missing_features).reset_index(drop=True)\n",
    "y_predict_bad_missing=y_predict_bad_missing.apply(lambda x: max(x,0.1))\n",
    "y_predict_bad_missing=y_predict_bad_missing[:len(bad_missing_data)]\n",
    "fig = px.line(x=bad_missing_data.index,y=bad_missing_data,title='OLD VS NEW MISSING AND BAD DATA(zoom to see overlap and changes)')\n",
    "fig.add_scatter(x=bad_missing_data.index,y=y_predict_bad_missing)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45341ec1-df90-4d0e-8048-f8bed0f9f337",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"BAD AND MISSING DAYS INITIAL: \")\n",
    "print(bad_missing_data.describe())\n",
    "print(\"BAD AND MISSING DAYS FINAL: \")\n",
    "print(y_predict_bad_missing.describe())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cec542-9a2a-49f0-b801-385093339bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"GOOD DAYS INITIAL: \")\n",
    "print(df_good_initial.describe())\n",
    "print(\"GOOD DAYS FINAL: \")\n",
    "print(df_good.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f15768d-5899-442f-8cf2-9f9d86ce53a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
